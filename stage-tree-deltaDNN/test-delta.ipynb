{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-20T12:59:05.967509Z",
     "start_time": "2025-09-20T12:26:15.883100Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# test_stage_tree.py\n",
    "import importlib, stage_runner  # ensures latest after edits\n",
    "importlib.reload(stage_runner)\n",
    "\n",
    "from stage_runner import StageTreeRunner, StaticCfg, Stage, Trial\n",
    "\n",
    "static = StaticCfg(\n",
    "    model_path=\"mlx-community/Mistral-7B-Instruct-v0.2-4bit\",\n",
    "    lora_layers=16,\n",
    "    val_batches=-1,\n",
    "    steps_per_eval=10,\n",
    "    dataset_id=\"youtube-comments-v1\",\n",
    "    tokenizer_id=\"mistral-tokenizer-v0.2\",\n",
    ")\n",
    "\n",
    "runner = StageTreeRunner(\n",
    "    static,\n",
    "    cache_dir=\"./stage_cache\",\n",
    "    lora_script=\"./scripts/lora.py\",\n",
    "    log_file=\"stage_log.csv\",\n",
    "    fastcdc_bin=None,   # or \"/path/to/fastcdc\" if you want chunk counts\n",
    ")\n",
    "\n",
    "# Two trials sharing stage 1 (HiPPO-style prefix reuse)\n",
    "trial_A = Trial(\"A_lr1e-5_then_5e-5\", [Stage(50, 1e-5), Stage(50, 5e-5)])\n",
    "trial_B = Trial(\"B_lr1e-5_then_3e-5\", [Stage(50, 1e-5), Stage(50, 3e-5)])\n",
    "\n",
    "runner.run_trial(trial_A)\n",
    "runner.run_trial(trial_B)\n",
    "print(\"\\nAll done. Metrics logged to stage_log.csv\")\n"
   ],
   "id": "dc9da596a40b0093",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[build] A_lr1e-5_then_5e-5 stage 1/2 (iters=50, lr=1e-05)\n",
      ">> python /Users/sanjeeb/Coding/HSSL/qlora-mlx/stage-tree-deltaDNN/scripts/lora.py --model mlx-community/Mistral-7B-Instruct-v0.2-4bit --train --iters 50 --steps-per-eval 10 --val-batches -1 --learning-rate 1e-05 --lora-layers 16 --adapter-file ./stage_cache/233ac6dad23f2688/adapters.npz\n",
      "Loading pretrained model\n",
      "\n",
      "Fetching 7 files:   0%|          | 0/7 [00:00<?, ?it/s]\n",
      "Fetching 7 files: 100%|██████████| 7/7 [00:00<00:00, 137840.98it/s]\n",
      "Total parameters 1243.189M\n",
      "Trainable parameters 0.852M\n",
      "Loading datasets\n",
      "Training\n",
      "Iter 1: Val loss 4.243, Val took 16.840s\n",
      "Iter 10: Train loss 4.008, It/sec 0.108, Tokens/sec 89.558\n",
      "Iter 10: Val loss 3.044, Val took 14.053s\n",
      "Iter 20: Train loss 2.690, It/sec 0.095, Tokens/sec 76.872\n",
      "Iter 20: Val loss 2.181, Val took 14.856s\n",
      "Iter 30: Train loss 1.690, It/sec 0.094, Tokens/sec 74.547\n",
      "Iter 30: Val loss 1.609, Val took 15.858s\n",
      "Iter 40: Train loss 1.352, It/sec 0.099, Tokens/sec 80.312\n",
      "Iter 40: Val loss 1.492, Val took 13.775s\n",
      "Iter 50: Train loss 1.347, It/sec 0.074, Tokens/sec 64.152\n",
      "Iter 50: Val loss 1.447, Val took 15.478s\n",
      "[build] A_lr1e-5_then_5e-5 stage 2/2 (iters=50, lr=5e-05)\n",
      ">> python /Users/sanjeeb/Coding/HSSL/qlora-mlx/stage-tree-deltaDNN/scripts/lora.py --model mlx-community/Mistral-7B-Instruct-v0.2-4bit --train --iters 50 --steps-per-eval 10 --val-batches -1 --learning-rate 5e-05 --lora-layers 16 --adapter-file ./stage_cache/6957581b2bb3213e/adapters.npz --resume-adapter-file ./stage_cache/233ac6dad23f2688/adapters.npz\n",
      "Loading pretrained model\n",
      "\n",
      "Fetching 7 files:   0%|          | 0/7 [00:00<?, ?it/s]\n",
      "Fetching 7 files: 100%|██████████| 7/7 [00:00<00:00, 131659.77it/s]\n",
      "Total parameters 1243.189M\n",
      "Trainable parameters 0.852M\n",
      "Loading datasets\n",
      "Loading pretrained adapters from ./stage_cache/233ac6dad23f2688/adapters.npz\n",
      "Training\n",
      "Iter 1: Val loss 1.476, Val took 19.131s\n",
      "Iter 10: Train loss 1.212, It/sec 0.098, Tokens/sec 81.065\n",
      "Iter 10: Val loss 1.372, Val took 15.322s\n",
      "Iter 20: Train loss 0.988, It/sec 0.083, Tokens/sec 67.168\n",
      "Iter 20: Val loss 1.418, Val took 14.290s\n",
      "Iter 30: Train loss 0.795, It/sec 0.087, Tokens/sec 69.071\n",
      "Iter 30: Val loss 1.561, Val took 17.307s\n",
      "Iter 40: Train loss 0.652, It/sec 0.096, Tokens/sec 77.548\n",
      "Iter 40: Val loss 1.674, Val took 13.993s\n",
      "Iter 50: Train loss 0.606, It/sec 0.074, Tokens/sec 64.236\n",
      "Iter 50: Val loss 1.757, Val took 15.071s\n",
      "[done] Trial A_lr1e-5_then_5e-5 final adapters at: ./stage_cache/6957581b2bb3213e/adapters.npz\n",
      "[build] B_lr1e-5_then_3e-5 stage 2/2 (iters=50, lr=3e-05)\n",
      ">> python /Users/sanjeeb/Coding/HSSL/qlora-mlx/stage-tree-deltaDNN/scripts/lora.py --model mlx-community/Mistral-7B-Instruct-v0.2-4bit --train --iters 50 --steps-per-eval 10 --val-batches -1 --learning-rate 3e-05 --lora-layers 16 --adapter-file ./stage_cache/aa5051b0d4f20a46/adapters.npz --resume-adapter-file ./stage_cache/233ac6dad23f2688/adapters.npz\n",
      "Loading pretrained model\n",
      "\n",
      "Fetching 7 files:   0%|          | 0/7 [00:00<?, ?it/s]\n",
      "Fetching 7 files: 100%|██████████| 7/7 [00:00<00:00, 97218.97it/s]\n",
      "Total parameters 1243.189M\n",
      "Trainable parameters 0.852M\n",
      "Loading datasets\n",
      "Loading pretrained adapters from ./stage_cache/233ac6dad23f2688/adapters.npz\n",
      "Training\n",
      "Iter 1: Val loss 1.452, Val took 15.862s\n",
      "Iter 10: Train loss 1.205, It/sec 0.102, Tokens/sec 84.676\n",
      "Iter 10: Val loss 1.388, Val took 14.425s\n",
      "Iter 20: Train loss 1.009, It/sec 0.100, Tokens/sec 81.176\n",
      "Iter 20: Val loss 1.414, Val took 13.781s\n",
      "Iter 30: Train loss 0.857, It/sec 0.090, Tokens/sec 71.702\n",
      "Iter 30: Val loss 1.525, Val took 15.321s\n",
      "Iter 40: Train loss 0.709, It/sec 0.099, Tokens/sec 79.767\n",
      "Iter 40: Val loss 1.605, Val took 13.648s\n",
      "Iter 50: Train loss 0.690, It/sec 0.083, Tokens/sec 72.069\n",
      "Iter 50: Val loss 1.645, Val took 13.854s\n",
      "[done] Trial B_lr1e-5_then_3e-5 final adapters at: ./stage_cache/aa5051b0d4f20a46/adapters.npz\n",
      "\n",
      "All done. Metrics logged to stage_log.csv\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-20T14:39:18.313161Z",
     "start_time": "2025-09-20T14:39:18.000601Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# analyze_results.py\n",
    "import pandas as pd\n",
    "from utils import fmt_bytes\n",
    "\n",
    "df = pd.read_csv(\"stage_log.csv\")\n",
    "\n",
    "# Basic cleanup/coercion\n",
    "for col in [\"stage_idx\",\"iters\",\"runtime_sec\",\"cache_hit\",\"cache_miss\",\n",
    "            \"size_full_bytes\",\"size_delta_bytes\",\"compression_ratio_full_over_delta\",\"lr\",\n",
    "            \"fastcdc_chunks\"]:\n",
    "    if col in df.columns:\n",
    "        df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "\n",
    "# 1) Per-trial stage table (like HiPPO)\n",
    "print(\"\\n=== Per-trial stages ===\")\n",
    "show = df[[\"trial\",\"stage_idx\",\"iters\",\"lr\",\"runtime_sec\",\"cache_hit\",\n",
    "           \"size_full_bytes\",\"size_delta_bytes\",\"compression_ratio_full_over_delta\"]].copy()\n",
    "# Prettify sizes\n",
    "show[\"size_full\"]  = show[\"size_full_bytes\"].apply(lambda x: fmt_bytes(int(x)) if pd.notna(x) else \"\")\n",
    "show[\"size_delta\"] = show[\"size_delta_bytes\"].apply(lambda x: fmt_bytes(int(x)) if pd.notna(x) else \"\")\n",
    "show = show.drop(columns=[\"size_full_bytes\",\"size_delta_bytes\"])\n",
    "print(show.to_string(index=False))\n",
    "\n",
    "# 2) Compute reuse: compare iters without reuse vs with reuse\n",
    "iters_built = df.loc[df[\"cache_hit\"]==0, \"iters\"].sum()\n",
    "iters_reused = df.loc[df[\"cache_hit\"]==1, \"iters\"].sum()\n",
    "iters_no_reuse = iters_built + iters_reused\n",
    "iters_with_reuse = iters_built\n",
    "saved_iters = iters_no_reuse - iters_with_reuse\n",
    "pct_saved = (saved_iters / iters_no_reuse * 100.0) if iters_no_reuse > 0 else 0.0\n",
    "speedup = (iters_no_reuse / iters_with_reuse) if iters_with_reuse > 0 else 1.0\n",
    "\n",
    "print(\"\\n=== Compute reuse summary ===\")\n",
    "print(f\"Iterations (no reuse baseline): {iters_no_reuse}\")\n",
    "print(f\"Iterations (with reuse):        {iters_with_reuse}\")\n",
    "print(f\"Saved iterations:               {saved_iters}  ({pct_saved:.2f}%)\")\n",
    "print(f\"Speedup:                        {speedup:.2f}×\")\n",
    "\n",
    "# 3) Storage: anchor vs delta\n",
    "print(\"\\n=== Storage savings (DeltaDNN-style) ===\")\n",
    "st2 = df[df[\"stage_idx\"]>1].copy()\n",
    "if len(st2) > 0:\n",
    "    total_full = st2[\"size_full_bytes\"].sum()\n",
    "    total_delta = st2[\"size_delta_bytes\"].sum()\n",
    "    ratio = (total_full / total_delta) if total_delta > 0 else float(\"inf\")\n",
    "    print(f\"Total full size (stages>1):  {fmt_bytes(int(total_full))}\")\n",
    "    print(f\"Total delta size (stages>1): {fmt_bytes(int(total_delta))}\")\n",
    "    print(f\"Aggregate compression ratio: {ratio:.2f}×\")\n",
    "else:\n",
    "    print(\"No stages >1 found; nothing to compress.\")\n",
    "\n",
    "# 4) Optional: FastCDC chunks per produced file\n",
    "if \"fastcdc_chunks\" in df.columns and df[\"fastcdc_chunks\"].notna().any():\n",
    "    print(\"\\n=== FastCDC chunk counts (optional) ===\")\n",
    "    print(df[[\"trial\",\"stage_idx\",\"fastcdc_chunks\",\"adapter_path\"]].to_string(index=False))\n"
   ],
   "id": "958bbebed728d15f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Per-trial stages ===\n",
      "             trial  stage_idx  iters      lr  runtime_sec  cache_hit  compression_ratio_full_over_delta size_full size_delta\n",
      "A_lr1e-5_then_5e-5          1     50 0.00001      650.219          0                           1.000000    3.3 MB      0.0 B\n",
      "A_lr1e-5_then_5e-5          2     50 0.00005      687.578          0                           1.986844    3.3 MB     1.6 MB\n",
      "B_lr1e-5_then_3e-5          1     50 0.00001        0.000          1                           1.000000    3.3 MB      0.0 B\n",
      "B_lr1e-5_then_3e-5          2     50 0.00003      632.073          0                           1.986844    3.3 MB     1.6 MB\n",
      "\n",
      "=== Compute reuse summary ===\n",
      "Iterations (no reuse baseline): 200\n",
      "Iterations (with reuse):        150\n",
      "Saved iterations:               50  (25.00%)\n",
      "Speedup:                        1.33×\n",
      "\n",
      "=== Storage savings (DeltaDNN-style) ===\n",
      "Total full size (stages>1):  6.5 MB\n",
      "Total delta size (stages>1): 3.3 MB\n",
      "Aggregate compression ratio: 1.99×\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Per-trial stage rows\n",
    "\n",
    "Each row is one “stage” of a trial. It is recorded: stage index, iterations, LR, how long that stage took, whether you *reused* a cached prefix (“cache_hit”), and two storage numbers inspired by DeltaDNN: the size of storing the entire adapter (“size_full”) vs. storing only the *delta* from the previous stage (“size_delta”). The derived metric `compression_ratio_full_over_delta = size_full / size_delta`.\n",
    "\n",
    "- **A, stage 1**\n",
    "    `cache_hit=0` and `size_delta=0.0 B` → First stage is the *base*, so there is nothing to delta against. You must store the full adapter once (≈3.3 MB). That’s why the row shows `compression_ratio = 1.0`—full vs delta is undefined here, so you pinned it to 1.0 for the base.\n",
    "\n",
    "- **A, stage 2**\n",
    "    `cache_hit=0` (built new) and `size_full≈3.3 MB`, `size_delta≈1.6 MB`.\n",
    "    `compression_ratio ≈ 3.3 / 1.66 ≈ 1.99×`.\n",
    "    Meaning: if you stored the raw adapter artifact for stage 2, it would be ~3.3 MB, but storing only the *difference* from stage 1 costs ~1.6 MB. That’s ~50% storage saved for this stage.\n",
    "\n",
    "- **B, stage 1**\n",
    "    `cache_hit=1` with `runtime_sec = 0.000` and `size_delta=0.0 B`.\n",
    "    This stage *reused* the cached prefix from trial A (same LR@50 iterations), so no retraining time and no new storage. That’s your stage-tree/HiPPO-style **compute reuse** in action.\n",
    "\n",
    "- **B, stage 2**\n",
    "    Built new from the reused stage-1 checkpoint (`cache_hit=0` for this stage), with the same storage pattern as A’s stage 2: full ≈3.3 MB vs delta ≈1.6 MB → ~1.99× compression.\n",
    "    Intuition: although LR differs (3e-5 vs 5e-5), the delta from the shared prefix is still about half the size of the full adapter, which is typical—later stages “nudge” weights rather than rewriting everything."
   ],
   "id": "f5663847e6e89251"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "cd8fa3c82163ccda"
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
