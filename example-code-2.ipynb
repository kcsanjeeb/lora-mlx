{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-14T01:58:33.518087Z",
     "start_time": "2025-09-14T01:58:33.515170Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ],
   "id": "34e605c01f8971c3",
   "outputs": [],
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "id": "88d29c77-e961-4717-b8b7-56f97ec9faf7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-13T12:44:25.488001Z",
     "start_time": "2025-09-13T12:44:22.520124Z"
    }
   },
   "source": [
    "import sys, os, subprocess, hashlib, torch\n",
    "from mlx_lm import load, generate"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "24f2aca0-9236-49a4-8a81-afa8d3a09771",
   "metadata": {},
   "source": [
    "#### 2. Run shell commands with live output\n",
    "* Runs any shell command.\n",
    "* Prints the output line by line instead of waiting until the end.\n",
    "* Very useful for training loops that take minutes to hours."
   ]
  },
  {
   "cell_type": "code",
   "id": "f590fd65-48bc-4a37-aa16-4e13fa3f59d9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-13T12:44:25.502796Z",
     "start_time": "2025-09-13T12:44:25.499895Z"
    }
   },
   "source": [
    "def run_command_with_live_output(command: list[str]) -> None:\n",
    "    \"\"\"\n",
    "    Courtesy of ChatGPT:\n",
    "    Runs a command and prints its output line by line as it executes.\n",
    "\n",
    "    Args:\n",
    "        command (List[str]): The command and its arguments to be executed.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "\n",
    "    # Print the output line by line\n",
    "    while True:\n",
    "        output = process.stdout.readline()\n",
    "        if output == '' and process.poll() is not None:\n",
    "            break\n",
    "        if output:\n",
    "            print(output.strip())\n",
    "        \n",
    "    # Print the error output, if any\n",
    "    err_output = process.stderr.read()\n",
    "    if err_output:\n",
    "        print(err_output)"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### 3. Format shell commands for easier copy/paste\n",
    "* Converts a Python list command into a clean string.\n",
    "* Example: `['python', 'scripts/convert.py', '--hf-path', 'model'] → \"python scripts/convert.py --hf-path model\"`"
   ],
   "id": "103020bed933842f"
  },
  {
   "cell_type": "code",
   "id": "48717683-138a-45c7-a1ac-ab43633ed124",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-13T12:44:25.511995Z",
     "start_time": "2025-09-13T12:44:25.509937Z"
    }
   },
   "source": [
    "def construct_shell_command(command: list[str]) -> str:\n",
    "    \n",
    "    return str(command).replace(\"'\",\"\").replace(\"[\",\"\").replace(\"]\",\"\").replace(\",\",\"\")"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### 4. Build prompts for testing inference\n",
    "* Defines a role prompt for instruction-tuned models.\n",
    "* Example: If a user says “Great content, thank you!”, the prompt tells the model how to respond like “ShawGPT”.\n",
    "* prompt_builder wraps user comments in this instruction format."
   ],
   "id": "d3e875a5199bec32"
  },
  {
   "cell_type": "code",
   "id": "2d75eca2-d427-4da9-aafa-bc3c4809c146",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-13T12:44:25.521568Z",
     "start_time": "2025-09-13T12:44:25.519327Z"
    }
   },
   "source": [
    "# prompt format\n",
    "intstructions_string = f\"\"\"ShawGPT, functioning as a virtual data science consultant on YouTube, communicates in clear, accessible language, escalating to technical depth upon request. \\\n",
    "It reacts to feedback aptly and ends responses with its signature '–ShawGPT'. \\\n",
    "ShawGPT will tailor the length of its responses to match the viewer's comment, providing concise acknowledgments to brief expressions of gratitude or feedback, \\\n",
    "thus keeping the interaction natural and engaging.\n",
    "\n",
    "Please respond to the following comment.\n",
    "\"\"\"\n",
    "\n",
    "prompt_builder = lambda comment: f'''<s>[INST] {intstructions_string} \\n{comment} \\n[/INST]\\n'''"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "id": "53a6447d-e900-4dc9-b4fe-95a57cf18ce7",
   "metadata": {},
   "source": [
    "#### 5. Convert Hugging Face model → MLX format / Quantize Model (optional)\n",
    "* Downloads Hugging Face Mistral model.\n",
    "* Converts it to MLX format (.npz files) for Apple Silicon.\n",
    "* -q quantizes the model → smaller & faster.\n",
    "* Prints the runnable command so you can also run it directly in the terminal."
   ]
  },
  {
   "cell_type": "code",
   "id": "e57299a3-7e87-4356-9edc-1a6dbb169305",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "ExecuteTime": {
     "end_time": "2025-09-13T12:44:25.531295Z",
     "start_time": "2025-09-13T12:44:25.529311Z"
    }
   },
   "source": [
    "hf_model_path = \"mistralai/Mistral-7B-Instruct-v0.2\""
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "da6079f6-3302-4737-a303-c3c99fc6f59b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "ExecuteTime": {
     "end_time": "2025-09-13T12:44:25.544637Z",
     "start_time": "2025-09-13T12:44:25.542703Z"
    }
   },
   "source": [
    "# define command to convert hf model to mlx format and save locally (-q flag quantizes model)\n",
    "command = ['python', 'scripts/convert.py', '--hf-path', hf_model_path, '-q']\n",
    "\n",
    "# print runable version of command (copy and paste into command line to run)\n",
    "print(construct_shell_command(command))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python scripts/convert.py --hf-path mistralai/Mistral-7B-Instruct-v0.2 -q\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "id": "6a75eb90-9106-4dce-a1a1-55582a9da80e",
   "metadata": {},
   "source": [
    "#### 6. Load quantized MLX model & test inference / Run inference with quantized model\n",
    "* Loads the 4-bit quantized Mistral model.\n",
    "* Builds a test prompt with prompt_builder.\n",
    "* Runs inference with generate.\n",
    "* max_tokens=140 → limits response length.\n",
    "* ✅ Baseline inference before fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "id": "09b88839-bf97-4cc2-805c-22f1d1d064b8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-13T12:44:25.562988Z",
     "start_time": "2025-09-13T12:44:25.560951Z"
    }
   },
   "source": [
    "model_path = \"mlx-community/Mistral-7B-Instruct-v0.2-4bit\"\n",
    "prompt = prompt_builder(\"Great content, thank you!\")\n",
    "max_tokens = 140"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "id": "92c82128-0069-4b34-860d-0a7dfb154b87",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-13T12:44:31.046183Z",
     "start_time": "2025-09-13T12:44:25.575687Z"
    }
   },
   "source": [
    "model, tokenizer = load(\"mlx-community/Mistral-7B-Instruct-v0.2-4bit\")\n",
    "response = generate(model, tokenizer, prompt=prompt, max_tokens = max_tokens,verbose=True)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Fetching 7 files:   0%|          | 0/7 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4e8e949779f34b959016b94f1ef7d922"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "–ShawGPT: I'm glad you're finding the content helpful and enjoyable! If you have any specific questions or topics you'd like me to cover in more depth, feel free to ask. Otherwise, I'll keep providing clear and accessible explanations for all things data science. Thanks for tuning in!\n",
      "==========\n",
      "Prompt: 121 tokens, 177.215 tokens-per-sec\n",
      "Generation: 69 tokens, 36.989 tokens-per-sec\n",
      "Peak memory: 4.547 GB\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "id": "cd99c836-e498-42de-9f6b-3f4d1a3507c0",
   "metadata": {},
   "source": "#### 7. Deduplication Helpers (CS + DR)"
  },
  {
   "cell_type": "code",
   "id": "b3bc7a1d-a738-4da3-8707-fa50c8d2887c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-14T01:43:32.629676Z",
     "start_time": "2025-09-14T01:43:32.622847Z"
    }
   },
   "source": [
    "forward_cache = {}  # for compute reuse\n",
    "fastcdc_path = \"/Users/sanjeeb/Coding/HSSL/fastcdc-go/cmd/fastcdc/fastcdc\"  # path to your binary\n",
    "\n",
    "def hash_batch(batch_tensor: torch.Tensor) -> str:\n",
    "    return hashlib.sha256(batch_tensor.cpu().numpy().tobytes()).hexdigest()\n",
    "\n",
    "def run_training_with_cache(num_iters=100, seed=None):\n",
    "    if seed is not None:\n",
    "        torch.manual_seed(seed)   # ensure same batches each run\n",
    "    hits, misses = 0, 0\n",
    "    for step in range(num_iters):\n",
    "        fake_batch = torch.randint(0, 1000, (8, 32))\n",
    "        h = hash_batch(fake_batch)\n",
    "        if h in forward_cache:\n",
    "            hits += 1\n",
    "        else:\n",
    "            forward_cache[h] = fake_batch.sum().item()\n",
    "            misses += 1\n",
    "    return hits, misses\n",
    "\n",
    "\n",
    "def run_fastcdc(input_file, output_file):\n",
    "    cmd = [fastcdc_path, \"-file\", input_file, \"-min\", \"16384\", \"-avg\", \"32768\", \"-max\", \"65536\"]\n",
    "    with open(output_file, \"w\") as f:\n",
    "        subprocess.run(cmd, stdout=f, stderr=subprocess.PIPE, text=True)\n",
    "    print(f\"✅ FastCDC finished: {input_file} → {output_file}\")\n",
    "\n",
    "def load_hashes(chunk_file):\n",
    "    hashes = []\n",
    "    with open(chunk_file) as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) >= 3:\n",
    "                hashes.append(parts[2])\n",
    "    return set(hashes)\n"
   ],
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 8. Hyperparameter Sweep with Compute + Storage Deduplication\n",
   "id": "2534a07eaef84164"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-14T01:26:41.247123Z",
     "start_time": "2025-09-14T01:26:41.237327Z"
    }
   },
   "cell_type": "code",
   "source": [
    "num_iters = \"100\"\n",
    "steps_per_eval = \"10\"\n",
    "val_batches = \"-1\"\n",
    "num_layers = 16\n",
    "\n",
    "learning_rates = [\"1e-5\", \"5e-5\"]\n",
    "results = []\n",
    "prev_hashes = None\n",
    "\n",
    "for lr in learning_rates:\n",
    "    adapter_file = f\"adapters_lr{lr}.npz\"\n",
    "    chunk_file = f\"chunks_lr{lr}.txt\"\n",
    "\n",
    "    if os.path.exists(adapter_file) and os.path.exists(chunk_file):\n",
    "        print(f\"Skipping fine-tuning for LR={lr}, artifacts already exist.\")\n",
    "    else:\n",
    "        print(f\"\\n=== Fine-tuning with LR={lr} ===\")\n",
    "\n",
    "        # LoRA fine-tuning run\n",
    "        command = [\n",
    "            'python', 'scripts/lora.py',\n",
    "            '--model', model_path,\n",
    "            '--train',\n",
    "            '--iters', num_iters,\n",
    "            '--steps-per-eval', steps_per_eval,\n",
    "            '--val-batches', val_batches,\n",
    "            '--learning-rate', lr,\n",
    "            '--lora-layers', str(num_layers)\n",
    "        ]\n",
    "        run_command_with_live_output(command)\n",
    "\n",
    "        # Save adapters\n",
    "        adapter_file = f\"adapters_lr{lr}.npz\"\n",
    "        os.rename(\"adapters.npz\", adapter_file)\n",
    "\n",
    "        # Run FastCDC\n",
    "        chunk_file = f\"chunks_lr{lr}.txt\"\n",
    "        run_fastcdc(adapter_file, chunk_file)\n",
    "\n",
    "        # Compute Savings (CS)\n",
    "        hits, misses = run_training_with_cache(num_iters=100, seed=42)\n",
    "\n",
    "        cs = hits / (hits + misses)\n",
    "        print(f\"Compute Savings (CS): {cs:.2%} (Hits={hits}, Misses={misses})\")\n",
    "\n",
    "        # Deduplication Ratio (DR)\n",
    "        hashes = load_hashes(chunk_file)\n",
    "        if prev_hashes is not None:\n",
    "            common = len(prev_hashes.intersection(hashes))\n",
    "            unique_curr = len(hashes - prev_hashes)\n",
    "            if (len(prev_hashes) + unique_curr) > 0:\n",
    "                dr = (len(prev_hashes) + len(hashes)) / (len(prev_hashes) + unique_curr)\n",
    "            else:\n",
    "                dr = 1.0\n",
    "            print(f\"Shared chunks: {common}, New unique chunks: {unique_curr}\")\n",
    "            print(f\"Deduplication Ratio (DR): {dr:.2f}\")\n",
    "        else:\n",
    "            dr = 1.00\n",
    "            print(\"Baseline run: DR = 1.00\")\n",
    "\n",
    "        results.append({\"lr\": lr, \"CS\": cs, \"DR\": dr})\n",
    "        prev_hashes = hashes\n"
   ],
   "id": "7cfb50be29f5492d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping fine-tuning for LR=1e-5, artifacts already exist.\n",
      "Skipping fine-tuning for LR=5e-5, artifacts already exist.\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Deduplication Analysis",
   "id": "8adabfdd7d5f289"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-14T01:58:38.781015Z",
     "start_time": "2025-09-14T01:58:38.723093Z"
    }
   },
   "cell_type": "code",
   "source": [
    "### Deduplication Analysis (Byte-level vs Value-level)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "learning_rates = [\"1e-5\", \"5e-5\"]\n",
    "results = []\n",
    "prev_hashes = None\n",
    "prev_adapter = None\n",
    "\n",
    "for lr in learning_rates:\n",
    "    print(f\"\\n=== Deduplication Analysis for LR={lr} ===\")\n",
    "\n",
    "    adapter_file = f\"adapters_lr{lr}.npz\"\n",
    "    chunk_file = f\"chunks_small_lr{lr}.txt\"\n",
    "\n",
    "    if not os.path.exists(adapter_file):\n",
    "        print(f\"❌ Missing adapter file: {adapter_file}\")\n",
    "        continue\n",
    "\n",
    "    # --- Run FastCDC again with smaller chunk sizes ---\n",
    "    cmd = [\n",
    "        fastcdc_path,\n",
    "        \"-file\", adapter_file,\n",
    "        \"-min\", \"1024\",  # 1 KB\n",
    "        \"-avg\", \"2048\",  # 2 KB\n",
    "        \"-max\", \"4096\"  # 4 KB\n",
    "    ]\n",
    "    with open(chunk_file, \"w\") as f:\n",
    "        subprocess.run(cmd, stdout=f, stderr=subprocess.PIPE, text=True)\n",
    "    print(f\"✅ Re-chunked {adapter_file} → {chunk_file}\")\n",
    "\n",
    "    # --- Compute Savings (CS) ---\n",
    "    hits, misses = run_training_with_cache(num_iters=100, seed=42)\n",
    "    cs = hits / (hits + misses) if (hits + misses) > 0 else 0.0\n",
    "    print(f\"Compute Savings (CS): {cs:.2%} (Hits={hits}, Misses={misses})\")\n",
    "\n",
    "    # --- Byte-level Deduplication (FastCDC DR) ---\n",
    "    hashes = load_hashes(chunk_file)\n",
    "    if prev_hashes is not None:\n",
    "        common = len(prev_hashes.intersection(hashes))\n",
    "        unique_curr = len(hashes - prev_hashes)\n",
    "        if (len(prev_hashes) + unique_curr) > 0:\n",
    "            dr_byte = (len(prev_hashes) + len(hashes)) / (len(prev_hashes) + unique_curr)\n",
    "        else:\n",
    "            dr_byte = 1.0\n",
    "        print(f\"[Byte-level] Shared chunks: {common}, New unique chunks: {unique_curr}, DR={dr_byte:.2f}\")\n",
    "    else:\n",
    "        dr_byte = 1.00\n",
    "        print(\"[Byte-level] Baseline run: DR = 1.00\")\n",
    "\n",
    "    # --- Value-level Similarity (NumPy arrays) ---\n",
    "    with np.load(adapter_file) as arrs:\n",
    "        if prev_adapter is not None:\n",
    "            diffs = []\n",
    "            for k in arrs.files:\n",
    "                if k in prev_adapter:\n",
    "                    diff = np.mean(np.abs(arrs[k] - prev_adapter[k]))\n",
    "                    diffs.append(diff)\n",
    "            mean_diff = float(np.mean(diffs)) if diffs else 0.0\n",
    "            print(f\"[Value-level] Mean weight difference: {mean_diff:.6f}\")\n",
    "        else:\n",
    "            mean_diff = 0.0\n",
    "            print(\"[Value-level] Baseline run (no comparison).\")\n",
    "\n",
    "    results.append({\"lr\": lr, \"CS\": cs, \"DR_byte\": dr_byte, \"MeanDiff_val\": mean_diff})\n",
    "    prev_hashes = hashes\n",
    "    prev_adapter = dict(np.load(adapter_file))\n",
    "\n",
    "# --- Show Results Table ---\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "print(\"\\n=== Results (Byte-level vs Value-level) ===\")\n",
    "print(df.to_string(index=False))\n"
   ],
   "id": "11135f11a1800907",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Deduplication Analysis for LR=1e-5 ===\n",
      "✅ Re-chunked adapters_lr1e-5.npz → chunks_small_lr1e-5.txt\n",
      "Compute Savings (CS): 100.00% (Hits=100, Misses=0)\n",
      "[Byte-level] Baseline run: DR = 1.00\n",
      "[Value-level] Baseline run (no comparison).\n",
      "\n",
      "=== Deduplication Analysis for LR=5e-5 ===\n",
      "✅ Re-chunked adapters_lr5e-5.npz → chunks_small_lr5e-5.txt\n",
      "Compute Savings (CS): 100.00% (Hits=100, Misses=0)\n",
      "[Byte-level] Shared chunks: 0, New unique chunks: 0, DR=1.00\n",
      "[Value-level] Mean weight difference: 0.006872\n",
      "\n",
      "=== Results (Byte-level vs Value-level) ===\n",
      "  lr  CS  DR_byte  MeanDiff_val\n",
      "1e-5 1.0      1.0      0.000000\n",
      "5e-5 1.0      1.0      0.006872\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "5933b45a6475343a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
