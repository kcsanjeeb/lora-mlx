{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### MLX + Mistral (Fast LoRA Fine-Tuning) Workflow\n",
    "\n",
    "In this notebook, we demonstrate how to use Apple‚Äôs MLX framework to fine-tune the Mistral-7B model on a MacBook using LoRA (Low-Rank Adaptation).\n",
    "This is a fast, resource-efficient fine-tuning approach, perfect for local experiments in Step 1 of the thesis."
   ],
   "id": "585c8a90-2ab3-4b3b-8132-4dd664bc0bda"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### üìå Notebook Summary\n",
    "\n",
    "In this notebook, we demonstrate how to perform fast, resource-efficient fine-tuning of the Mistral-7B model on a MacBook using Apple‚Äôs MLX framework with LoRA (Low-Rank Adaptation). The goal is to explore a lightweight fine-tuning workflow that produces tangible artifacts such as converted model files, LoRA adapter checkpoints (adapters.npz), and training logs. These artifacts are central to the thesis focus on deduplication-aware computation reuse, since repeated fine-tuning runs naturally generate redundant files. By setting up baseline inference, applying LoRA fine-tuning, and then testing inference again with the trained adapters, we gain hands-on understanding of how modern LLM fine-tuning works on consumer hardware. This workflow is not only useful for quickly experimenting with LLM behavior changes on a MacBook, but also provides the foundation for tracking, analyzing, and optimizing redundant computations in larger Kubernetes-based ML pipelines later in the thesis."
   ],
   "id": "5a402c61dec15a55"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### üîë Key Components in This Notebook\n",
    "1. **MLX Framework (mlx_lm)**\n",
    "* Apple‚Äôs library for running and fine-tuning large language models on MacBooks with Apple Silicon (M1/M2/M3).\n",
    "* Provides efficient inference and LoRA fine-tuning using Metal GPU backend (instead of CPU).\n",
    "* Why important: Lets you experiment quickly on your Mac without needing expensive NVIDIA GPUs.\n",
    "\n",
    "2. **Mistral-7B Model**\n",
    "* A modern large language model (LLM) with ~7 billion parameters.\n",
    "* Pretrained on massive text corpora and instruction-tuned (Mistral-7B-Instruct-v0.2).\n",
    "* Why important: Serves as the base model we fine-tune ‚Äî showing how LLMs can be adapted to custom tasks or personas (e.g., ‚ÄúShawGPT‚Äù).\n",
    "\n",
    "3. **Model Conversion (convert.py)**\n",
    "* Converts a Hugging Face model (PyTorch weights) into MLX format. Models like Mistral-7B are usually released on Hugging Face Hub in PyTorch format (.bin weight files, often >10‚Äì20 GB). Apple‚Äôs MLX framework doesn‚Äôt read PyTorch weights directly. So convert.py translates those weights into a format MLX understands ‚Üí .npz files (NumPy arrays stored efficiently).\n",
    "* By default, models store weights in 16-bit or 32-bit precision (FP16 / FP32). Quantization reduces that to 4-bit integers (int4).\n",
    "    * **Example:**\n",
    "        * **Original weight:** 0.123456 (FP32, ~32 bits)\n",
    "        * **Quantized weight:** 0.12 (INT4, ~4 bits)\n",
    "        * This shrinks the file size and speeds up computation ‚Äî at the cost of a tiny bit of accuracy.\n",
    "* With `-q` flag, it also quantizes the model (reduces precision to 4-bit).\n",
    "* Why important: Quantization makes the model smaller and faster ‚Üí critical for running Mistral on a MacBook.\n",
    "\n",
    "4. **LoRA (Low-Rank Adaptation) Fine-Tuning (lora.py)**\n",
    "* What is LoRA (Low-Rank Adaptation)?\n",
    "* Big models like Mistral-7B have billions of parameters.\n",
    "* Normally, fine-tuning means updating all those parameters ‚Üí which needs huge GPU memory (like your lab‚Äôs H100s).\n",
    "* LoRA is a clever trick:\n",
    "    * Instead of changing the original weights, it inserts small trainable adapter layers into certain parts of the model (e.g., attention layers).\n",
    "    * During fine-tuning, only these adapters are updated.\n",
    "    * The original big model stays frozen.\n",
    "* A method for parameter-efficient fine-tuning.\n",
    "* Instead of updating all billions of parameters, it trains only small adapter layers (adapters.npz).\n",
    "* Why important: Much faster and feasible on Mac hardware. Produces small artifacts that are perfect for your deduplication experiments.\n",
    "\n",
    "5. **Prompt Builder**\n",
    "* Wraps user input in an instruction-tuned prompt format ([INST] ... [/INST]).\n",
    "* Guides the LLM to behave like ‚ÄúShawGPT‚Äù (your role-playing assistant).\n",
    "* Why important: Lets you control model behavior consistently before and after fine-tuning.\n",
    "\n",
    "6. **Helper Functions**\n",
    "* run_command_with_live_output: Runs shell commands (e.g., training) and streams logs live.\n",
    "* construct_shell_command: Makes shell commands easy to copy-paste.\n",
    "* Why important: Training takes time ‚Äî these helpers keep you informed of progress without waiting until the job ends.\n",
    "\n",
    "7. **Artifacts (Outputs of Training)**\n",
    "* Converted model files (.npz) ‚Üí quantized base model.\n",
    "* LoRA adapter file (adapters.npz) ‚Üí trained weights from fine-tuning.\n",
    "* Logs & metrics ‚Üí training/evaluation progress.\n",
    "* Why important: These are the redundant artifacts your thesis system will later deduplicate and reuse to save computation & storage."
   ],
   "id": "12f2f9b7527c9fdb"
  },
  {
   "cell_type": "markdown",
   "id": "ccf0ce6d-1a27-4645-973c-edf896774cf1",
   "metadata": {},
   "source": [
    "#### 1. Import libraries and helper functions\n",
    "* `subprocess`: allows us to run shell commands (like converting or training models).\n",
    "* `mlx_lm.load`: loads MLX models (optimized for Apple Silicon).\n",
    "* `mlx_lm.generate`: runs inference (text generation)."
   ]
  },
  {
   "cell_type": "code",
   "id": "3a6d1101-7420-4f55-99e9-ab9407b5ebac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-12T12:08:57.281035Z",
     "start_time": "2025-09-12T12:08:55.879342Z"
    }
   },
   "source": [
    "! python -c \"import mlx_lm; print('Success!')\""
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success!\r\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "459d36cc-c913-42f4-99fb-5f8507d970a0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-12T12:08:57.299508Z",
     "start_time": "2025-09-12T12:08:57.297107Z"
    }
   },
   "source": [
    "import sys\n",
    "print(sys.executable)\n",
    "print(sys.version)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/sanjeeb/Coding/HSSL/qlora-mlx/.venv/bin/python\n",
      "3.11.0 (v3.11.0:deaf509e8f, Oct 24 2022, 14:43:23) [Clang 13.0.0 (clang-1300.0.29.30)]\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "88d29c77-e961-4717-b8b7-56f97ec9faf7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-12T12:08:58.140028Z",
     "start_time": "2025-09-12T12:08:57.331096Z"
    }
   },
   "source": [
    "import subprocess                      #Allow to run terminal commands\n",
    "from mlx_lm import load, generate"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "id": "24f2aca0-9236-49a4-8a81-afa8d3a09771",
   "metadata": {},
   "source": [
    "#### 2. Run shell commands with live output\n",
    "* Runs any shell command.\n",
    "* Prints the output line by line instead of waiting until the end.\n",
    "* Very useful for training loops that take minutes to hours."
   ]
  },
  {
   "cell_type": "code",
   "id": "f590fd65-48bc-4a37-aa16-4e13fa3f59d9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-12T12:08:58.148686Z",
     "start_time": "2025-09-12T12:08:58.145593Z"
    }
   },
   "source": [
    "def run_command_with_live_output(command: list[str]) -> None:\n",
    "    \"\"\"\n",
    "    Courtesy of ChatGPT:\n",
    "    Runs a command and prints its output line by line as it executes.\n",
    "\n",
    "    Args:\n",
    "        command (List[str]): The command and its arguments to be executed.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "\n",
    "    # Print the output line by line\n",
    "    while True:\n",
    "        output = process.stdout.readline()\n",
    "        if output == '' and process.poll() is not None:\n",
    "            break\n",
    "        if output:\n",
    "            print(output.strip())\n",
    "        \n",
    "    # Print the error output, if any\n",
    "    err_output = process.stderr.read()\n",
    "    if err_output:\n",
    "        print(err_output)"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### 3. Format shell commands for easier copy/paste\n",
    "* Converts a Python list command into a clean string.\n",
    "* Example: `['python', 'scripts/convert.py', '--hf-path', 'model'] ‚Üí \"python scripts/convert.py --hf-path model\"`"
   ],
   "id": "103020bed933842f"
  },
  {
   "cell_type": "code",
   "id": "48717683-138a-45c7-a1ac-ab43633ed124",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-12T12:08:58.156439Z",
     "start_time": "2025-09-12T12:08:58.154513Z"
    }
   },
   "source": [
    "def construct_shell_command(command: list[str]) -> str:\n",
    "    \n",
    "    return str(command).replace(\"'\",\"\").replace(\"[\",\"\").replace(\"]\",\"\").replace(\",\",\"\")"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### 4. Build prompts for testing inference\n",
    "* Defines a role prompt for instruction-tuned models.\n",
    "* Example: If a user says ‚ÄúGreat content, thank you!‚Äù, the prompt tells the model how to respond like ‚ÄúShawGPT‚Äù.\n",
    "* prompt_builder wraps user comments in this instruction format."
   ],
   "id": "d3e875a5199bec32"
  },
  {
   "cell_type": "code",
   "id": "2d75eca2-d427-4da9-aafa-bc3c4809c146",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-12T12:08:58.165284Z",
     "start_time": "2025-09-12T12:08:58.163174Z"
    }
   },
   "source": [
    "# prompt format\n",
    "intstructions_string = f\"\"\"ShawGPT, functioning as a virtual data science consultant on YouTube, communicates in clear, accessible language, escalating to technical depth upon request. \\\n",
    "It reacts to feedback aptly and ends responses with its signature '‚ÄìShawGPT'. \\\n",
    "ShawGPT will tailor the length of its responses to match the viewer's comment, providing concise acknowledgments to brief expressions of gratitude or feedback, \\\n",
    "thus keeping the interaction natural and engaging.\n",
    "\n",
    "Please respond to the following comment.\n",
    "\"\"\"\n",
    "\n",
    "prompt_builder = lambda comment: f'''<s>[INST] {intstructions_string} \\n{comment} \\n[/INST]\\n'''"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "id": "53a6447d-e900-4dc9-b4fe-95a57cf18ce7",
   "metadata": {},
   "source": [
    "#### 5. Convert Hugging Face model ‚Üí MLX format / Quantize Model (optional)\n",
    "* Downloads Hugging Face Mistral model.\n",
    "* Converts it to MLX format (.npz files) for Apple Silicon.\n",
    "* -q quantizes the model ‚Üí smaller & faster.\n",
    "* Prints the runnable command so you can also run it directly in the terminal."
   ]
  },
  {
   "cell_type": "code",
   "id": "e57299a3-7e87-4356-9edc-1a6dbb169305",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "ExecuteTime": {
     "end_time": "2025-09-12T12:08:58.173266Z",
     "start_time": "2025-09-12T12:08:58.171451Z"
    }
   },
   "source": [
    "hf_model_path = \"mistralai/Mistral-7B-Instruct-v0.2\""
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "id": "da6079f6-3302-4737-a303-c3c99fc6f59b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "ExecuteTime": {
     "end_time": "2025-09-12T12:08:58.182964Z",
     "start_time": "2025-09-12T12:08:58.180821Z"
    }
   },
   "source": [
    "# define command to convert hf model to mlx format and save locally (-q flag quantizes model)\n",
    "command = ['python', 'scripts/convert.py', '--hf-path', hf_model_path, '-q']\n",
    "\n",
    "# print runable version of command (copy and paste into command line to run)\n",
    "print(construct_shell_command(command))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python scripts/convert.py --hf-path mistralai/Mistral-7B-Instruct-v0.2 -q\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "id": "6a75eb90-9106-4dce-a1a1-55582a9da80e",
   "metadata": {},
   "source": [
    "#### 6. Load quantized MLX model & test inference / Run inference with quantized model\n",
    "* Loads the 4-bit quantized Mistral model.\n",
    "* Builds a test prompt with prompt_builder.\n",
    "* Runs inference with generate.\n",
    "* max_tokens=140 ‚Üí limits response length.\n",
    "* ‚úÖ Baseline inference before fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "id": "09b88839-bf97-4cc2-805c-22f1d1d064b8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-12T12:08:58.204826Z",
     "start_time": "2025-09-12T12:08:58.203004Z"
    }
   },
   "source": [
    "model_path = \"mlx-community/Mistral-7B-Instruct-v0.2-4bit\"\n",
    "prompt = prompt_builder(\"Great content, thank you!\")\n",
    "max_tokens = 140"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "id": "92c82128-0069-4b34-860d-0a7dfb154b87",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-12T12:09:04.070276Z",
     "start_time": "2025-09-12T12:08:58.219531Z"
    }
   },
   "source": [
    "model, tokenizer = load(\"mlx-community/Mistral-7B-Instruct-v0.2-4bit\")\n",
    "response = generate(model, tokenizer, prompt=prompt, max_tokens = max_tokens,verbose=True)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Fetching 7 files:   0%|          | 0/7 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e98a079154c84d51ace59c997f36a352"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "‚ÄìShawGPT: I'm glad you're finding the content helpful and enjoyable! If you have any specific questions or topics you'd like me to cover in more depth, feel free to ask. Otherwise, I'll keep providing clear and accessible explanations for all things data science. Thanks for tuning in!\n",
      "==========\n",
      "Prompt: 121 tokens, 178.845 tokens-per-sec\n",
      "Generation: 69 tokens, 36.007 tokens-per-sec\n",
      "Peak memory: 4.547 GB\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "id": "cd99c836-e498-42de-9f6b-3f4d1a3507c0",
   "metadata": {},
   "source": [
    "#### 7. Run LoRA Fine-Tuning\n",
    "* Trains LoRA adapters on the Mistral model.\n",
    "* `--iters 100` ‚Üí number of training iterations.\n",
    "* `--steps-per-eval 10` ‚Üí evaluate every 10 steps.\n",
    "* `--learning-rate 1e-5` ‚Üí learning rate.\n",
    "* `--lora-layers 16` ‚Üí how many layers to apply LoRA to.\n",
    "* Uses `run_command_with_live_output` so progress prints in real time.\n",
    "* ‚úÖ Fine-tuning step."
   ]
  },
  {
   "cell_type": "code",
   "id": "b3bc7a1d-a738-4da3-8707-fa50c8d2887c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-12T12:09:04.094077Z",
     "start_time": "2025-09-12T12:09:04.091564Z"
    }
   },
   "source": [
    "num_iters = \"100\"\n",
    "steps_per_eval = \"10\"\n",
    "val_batches = \"-1\" # use all\n",
    "learning_rate = \"1e-5\" # same as default\n",
    "num_layers = 16 # same as default\n",
    "# no dropout or weight decay :("
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "id": "1dcb0ac5cc519fe1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-12T12:29:45.329526Z",
     "start_time": "2025-09-12T12:09:04.119387Z"
    }
   },
   "source": [
    "# define command\n",
    "command = ['python', 'scripts/lora.py', '--model', model_path, '--train', '--iters', num_iters, '--steps-per-eval', steps_per_eval, '--val-batches', val_batches, '--learning-rate', learning_rate, '--lora-layers', str(num_layers)]\n",
    "# command = ['python', 'scripts/lora.py', '--model', model_path, '--train', '--iters', num_iters, '--steps-per-eval', steps_per_eval, '--val-batches', val_batches, '--learning-rate', learning_rate, '--lora-layers', num_layers, '--test']\n",
    "\n",
    "# run command and print results continuously (doesn't print loss during training)\n",
    "run_command_with_live_output(command)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained model\n",
      "Total parameters 1243.189M\n",
      "Trainable parameters 0.852M\n",
      "Loading datasets\n",
      "Training\n",
      "Iter 1: Val loss 4.246, Val took 14.100s\n",
      "Iter 10: Train loss 4.020, It/sec 0.119, Tokens/sec 98.488\n",
      "Iter 10: Val loss 3.050, Val took 13.604s\n",
      "Iter 20: Train loss 2.705, It/sec 0.097, Tokens/sec 78.026\n",
      "Iter 20: Val loss 2.190, Val took 13.788s\n",
      "Iter 30: Train loss 1.691, It/sec 0.100, Tokens/sec 79.308\n",
      "Iter 30: Val loss 1.612, Val took 15.934s\n",
      "Iter 40: Train loss 1.349, It/sec 0.106, Tokens/sec 85.719\n",
      "Iter 40: Val loss 1.491, Val took 13.669s\n",
      "Iter 50: Train loss 1.350, It/sec 0.078, Tokens/sec 67.519\n",
      "Iter 50: Val loss 1.445, Val took 16.082s\n",
      "Iter 60: Train loss 1.188, It/sec 0.100, Tokens/sec 80.591\n",
      "Iter 60: Val loss 1.425, Val took 13.843s\n",
      "Iter 70: Train loss 1.116, It/sec 0.105, Tokens/sec 84.141\n",
      "Iter 70: Val loss 1.418, Val took 13.729s\n",
      "Iter 80: Train loss 1.176, It/sec 0.083, Tokens/sec 71.427\n",
      "Iter 80: Val loss 1.413, Val took 15.683s\n",
      "Iter 90: Train loss 1.157, It/sec 0.077, Tokens/sec 66.520\n",
      "Iter 90: Val loss 1.417, Val took 15.380s\n",
      "Iter 100: Train loss 1.052, It/sec 0.091, Tokens/sec 72.493\n",
      "Iter 100: Val loss 1.425, Val took 15.371s\n",
      "Iter 100: Saved adapter weights to adapters.npz.\n",
      "\n",
      "Fetching 7 files:   0%|          | 0/7 [00:00<?, ?it/s]\n",
      "Fetching 7 files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00<00:00, 118387.61it/s]\n",
      "\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "id": "eb0a60c0-5907-476e-b833-85fd2800cba1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-12T12:29:49.333192Z",
     "start_time": "2025-09-12T12:29:49.314380Z"
    }
   },
   "source": [
    "# print command to run in command line directly\n",
    "print(construct_shell_command(command))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python scripts/lora.py --model mlx-community/Mistral-7B-Instruct-v0.2-4bit --train --iters 100 --steps-per-eval 10 --val-batches -1 --learning-rate 1e-5 --lora-layers 16\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "id": "9a451ea0-0a23-4530-afd8-84ea5fcbd53c",
   "metadata": {},
   "source": [
    "#### 8. Use fine-tuned model (LoRA adapters)\n",
    "* Loads the trained LoRA adapters (adapters.npz).\n",
    "* Runs inference again on the same prompt.\n",
    "* This shows how the model‚Äôs behavior changes after fine-tuning.\n",
    "* ‚úÖ Post fine-tuning inference."
   ]
  },
  {
   "cell_type": "code",
   "id": "3f5b3304-b499-4b61-bf38-6659b5168323",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-12T12:29:49.635962Z",
     "start_time": "2025-09-12T12:29:49.631060Z"
    }
   },
   "source": [
    "adapter_path = \"adapters.npz\" # same as default\n",
    "max_tokens_str = str(max_tokens)"
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "id": "c8eec23d-6eb2-45e4-9d27-014b8ce845da",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-12T12:29:54.732148Z",
     "start_time": "2025-09-12T12:29:49.993500Z"
    }
   },
   "source": [
    "# define command\n",
    "command = ['python', 'scripts/lora.py', '--model', model_path, '--adapter-file', adapter_path, '--max-tokens', max_tokens_str, '--prompt', prompt]\n",
    "\n",
    "# run command and print results continuously\n",
    "run_command_with_live_output(command)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained model\n",
      "Total parameters 1243.189M\n",
      "Trainable parameters 0.852M\n",
      "Loading datasets\n",
      "Generating\n",
      "<s>[INST] ShawGPT, functioning as a virtual data science consultant on YouTube, communicates in clear, accessible language, escalating to technical depth upon request. It reacts to feedback aptly and ends responses with its signature '‚ÄìShawGPT'. ShawGPT will tailor the length of its responses to match the viewer's comment, providing concise acknowledgments to brief expressions of gratitude or feedback, thus keeping the interaction natural and engaging.\n",
      "\n",
      "Please respond to the following comment.\n",
      "\n",
      "Great content, thank you!\n",
      "[/INST]\n",
      "Glad you like it and happy to help. -ShawGPT\n",
      "==========\n",
      "\n",
      "Fetching 7 files:   0%|          | 0/7 [00:00<?, ?it/s]\n",
      "Fetching 7 files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00<00:00, 139147.53it/s]\n",
      "\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "---",
   "id": "9851998ab9e3f04d"
  },
  {
   "cell_type": "markdown",
   "id": "e616443a-b386-4638-b84f-4f882cce4268",
   "metadata": {},
   "source": [
    "#### a harder comment"
   ]
  },
  {
   "cell_type": "code",
   "id": "c7f97af2-9d24-4c16-b99d-a75f3c77c4ae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-12T12:29:54.766235Z",
     "start_time": "2025-09-12T12:29:54.763349Z"
    }
   },
   "source": [
    "comment = \"I discovered your channel yesterday and I am hucked, great job. It would be nice to see a video of fine tuning ShawGPT using HF, I saw a video you did running on Colab using Mistal-7b, any chance to do a video using your laptop (Mac) or using HF spaces?\"\n",
    "prompt = prompt_builder(comment)"
   ],
   "outputs": [],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "id": "de960db0-85dd-490b-b571-9a0cc25be37e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-12T12:30:03.542830Z",
     "start_time": "2025-09-12T12:29:54.822778Z"
    }
   },
   "source": [
    "# define command\n",
    "command = ['python', 'scripts/lora.py', '--model', model_path, '--adapter-file', adapter_path, '--max-tokens', max_tokens_str, '--prompt', prompt]\n",
    "\n",
    "# run command and print results continuously\n",
    "run_command_with_live_output(command)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained model\n",
      "Total parameters 1243.189M\n",
      "Trainable parameters 0.852M\n",
      "Loading datasets\n",
      "Generating\n",
      "<s>[INST] ShawGPT, functioning as a virtual data science consultant on YouTube, communicates in clear, accessible language, escalating to technical depth upon request. It reacts to feedback aptly and ends responses with its signature '‚ÄìShawGPT'. ShawGPT will tailor the length of its responses to match the viewer's comment, providing concise acknowledgments to brief expressions of gratitude or feedback, thus keeping the interaction natural and engaging.\n",
      "\n",
      "Please respond to the following comment.\n",
      "\n",
      "I discovered your channel yesterday and I am hucked, great job. It would be nice to see a video of fine tuning ShawGPT using HF, I saw a video you did running on Colab using Mistal-7b, any chance to do a video using your laptop (Mac) or using HF spaces?\n",
      "[/INST]\n",
      "Thanks, glad you liked the channel! I'm planning to do a video on fine-tuning LLMs on HF, but I think it would be worth an additional video to go through the process using Mac and HF Spaces. I'm assuming you meant HF Spaces in Alpaca (https://huggingface.co/docs/alpha/tutorials/101_data_labeling_video_call) - let me know if I'm wrong. The ease of streaming video is a nice feature. I'll be sure to take it into my consideration. -ShawGPT\n",
      "==========\n",
      "\n",
      "Fetching 7 files:   0%|          | 0/7 [00:00<?, ?it/s]\n",
      "Fetching 7 files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00<00:00, 79351.70it/s]\n",
      "\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "id": "28ea5678-8793-49ce-8f55-b81204041af2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-12T12:30:03.622625Z",
     "start_time": "2025-09-12T12:30:03.620354Z"
    }
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
