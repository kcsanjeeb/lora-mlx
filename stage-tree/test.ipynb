{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Stage-Based Compute Reuse Experiment\n",
    "\n",
    "### Setup\n",
    "We ran two fine-tuning trials on **Mistral-7B (LoRA)** using a stage-based execution model.\n",
    "\n",
    "- **Trial A:** 50 iters @ LR=1e-5 → 50 iters @ LR=5e-5\n",
    "- **Trial B:** 50 iters @ LR=1e-5 → 50 iters @ LR=3e-5\n",
    "\n",
    "The StageTreeRunner cached checkpoints for stage prefixes and reused them across trials."
   ],
   "id": "35c739fbb45e009c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-20T09:59:10.105877Z",
     "start_time": "2025-09-20T09:28:20.289236Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import importlib, stage_runner\n",
    "importlib.reload(stage_runner)\n",
    "\n",
    "from stage_runner import StageTreeRunner, StaticCfg, Stage, Trial\n",
    "\n",
    "static = StaticCfg(\n",
    "    model_path=\"mlx-community/Mistral-7B-Instruct-v0.2-4bit\",\n",
    "    lora_layers=16,\n",
    "    val_batches=-1,\n",
    "    steps_per_eval=10,\n",
    "    dataset_id=\"youtube-comments-v1\",\n",
    "    tokenizer_id=\"mistral-tokenizer-v0.2\",\n",
    ")\n",
    "runner = StageTreeRunner(static, cache_dir=\"./stage_cache\")\n",
    "\n",
    "# Define trials as stage sequences (warmup -> fine)\n",
    "# Trial A: 50 iters @ 1e-5 then 50 @ 5e-5\n",
    "\n",
    "trial_A = Trial(\"A_lr1e-5_then_5e-5\", [Stage(50,1e-5), Stage(50,5e-5)])\n",
    "trial_B = Trial(\"B_lr1e-5_then_3e-5\", [Stage(50,1e-5), Stage(50,3e-5)])\n",
    "\n",
    "runner.run_trial(trial_A)  # builds both stages\n",
    "runner.run_trial(trial_B)  # reuses stage 1 from A, trains only stage 2"
   ],
   "id": "2327f281c3698d1c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[build] A_lr1e-5_then_5e-5 stage 1/2 (iters=50, lr=1e-05)\n",
      ">> python /Users/sanjeeb/Coding/HSSL/qlora-mlx/stage-tree/scripts/lora.py --model mlx-community/Mistral-7B-Instruct-v0.2-4bit --train --iters 50 --steps-per-eval 10 --val-batches -1 --learning-rate 1e-05 --lora-layers 16 --adapter-file ./stage_cache/233ac6dad23f2688/adapters.npz\n",
      "Loading pretrained model\n",
      "\n",
      "Fetching 7 files:   0%|          | 0/7 [00:00<?, ?it/s]\n",
      "Fetching 7 files: 100%|██████████| 7/7 [00:00<00:00, 93503.59it/s]\n",
      "Total parameters 1243.189M\n",
      "Trainable parameters 0.852M\n",
      "Loading datasets\n",
      "Training\n",
      "Iter 1: Val loss 4.250, Val took 15.061s\n",
      "Iter 10: Train loss 4.034, It/sec 0.113, Tokens/sec 93.701\n",
      "Iter 10: Val loss 3.076, Val took 13.949s\n",
      "Iter 20: Train loss 2.774, It/sec 0.099, Tokens/sec 80.333\n",
      "Iter 20: Val loss 2.292, Val took 14.754s\n",
      "Iter 30: Train loss 1.778, It/sec 0.097, Tokens/sec 77.261\n",
      "Iter 30: Val loss 1.633, Val took 15.656s\n",
      "Iter 40: Train loss 1.363, It/sec 0.104, Tokens/sec 84.485\n",
      "Iter 40: Val loss 1.507, Val took 14.096s\n",
      "Iter 50: Train loss 1.358, It/sec 0.090, Tokens/sec 77.769\n",
      "Iter 50: Val loss 1.457, Val took 13.685s\n",
      "[saved] ./stage_cache/233ac6dad23f2688/adapters.npz\n",
      "[build] A_lr1e-5_then_5e-5 stage 2/2 (iters=50, lr=5e-05)\n",
      ">> python /Users/sanjeeb/Coding/HSSL/qlora-mlx/stage-tree/scripts/lora.py --model mlx-community/Mistral-7B-Instruct-v0.2-4bit --train --iters 50 --steps-per-eval 10 --val-batches -1 --learning-rate 5e-05 --lora-layers 16 --adapter-file ./stage_cache/6957581b2bb3213e/adapters.npz --resume-adapter-file ./stage_cache/233ac6dad23f2688/adapters.npz\n",
      "Loading pretrained model\n",
      "\n",
      "Fetching 7 files:   0%|          | 0/7 [00:00<?, ?it/s]\n",
      "Fetching 7 files: 100%|██████████| 7/7 [00:00<00:00, 93206.76it/s]\n",
      "Total parameters 1243.189M\n",
      "Trainable parameters 0.852M\n",
      "Loading datasets\n",
      "Loading pretrained adapters from ./stage_cache/233ac6dad23f2688/adapters.npz\n",
      "Training\n",
      "Iter 1: Val loss 1.472, Val took 13.974s\n",
      "Iter 10: Train loss 1.235, It/sec 0.116, Tokens/sec 96.130\n",
      "Iter 10: Val loss 1.395, Val took 13.789s\n",
      "Iter 20: Train loss 1.019, It/sec 0.092, Tokens/sec 74.551\n",
      "Iter 20: Val loss 1.396, Val took 14.327s\n",
      "Iter 30: Train loss 0.820, It/sec 0.094, Tokens/sec 75.217\n",
      "Iter 30: Val loss 1.549, Val took 15.123s\n",
      "Iter 40: Train loss 0.664, It/sec 0.099, Tokens/sec 80.305\n",
      "Iter 40: Val loss 1.808, Val took 14.042s\n",
      "Iter 50: Train loss 0.628, It/sec 0.086, Tokens/sec 74.391\n",
      "Iter 50: Val loss 1.805, Val took 13.787s\n",
      "[saved] ./stage_cache/6957581b2bb3213e/adapters.npz\n",
      "[done] Trial A_lr1e-5_then_5e-5 final adapters at: ./stage_cache/6957581b2bb3213e/adapters.npz\n",
      "[cache hit] B_lr1e-5_then_3e-5 prefix 1/2 → ./stage_cache/233ac6dad23f2688/adapters.npz\n",
      "[build] B_lr1e-5_then_3e-5 stage 2/2 (iters=50, lr=3e-05)\n",
      ">> python /Users/sanjeeb/Coding/HSSL/qlora-mlx/stage-tree/scripts/lora.py --model mlx-community/Mistral-7B-Instruct-v0.2-4bit --train --iters 50 --steps-per-eval 10 --val-batches -1 --learning-rate 3e-05 --lora-layers 16 --adapter-file ./stage_cache/aa5051b0d4f20a46/adapters.npz --resume-adapter-file ./stage_cache/233ac6dad23f2688/adapters.npz\n",
      "Loading pretrained model\n",
      "\n",
      "Fetching 7 files:   0%|          | 0/7 [00:00<?, ?it/s]\n",
      "Fetching 7 files: 100%|██████████| 7/7 [00:00<00:00, 145347.17it/s]\n",
      "Total parameters 1243.189M\n",
      "Trainable parameters 0.852M\n",
      "Loading datasets\n",
      "Loading pretrained adapters from ./stage_cache/233ac6dad23f2688/adapters.npz\n",
      "Training\n",
      "Iter 1: Val loss 1.457, Val took 14.932s\n",
      "Iter 10: Train loss 1.221, It/sec 0.115, Tokens/sec 94.997\n",
      "Iter 10: Val loss 1.401, Val took 13.927s\n",
      "Iter 20: Train loss 1.068, It/sec 0.098, Tokens/sec 79.124\n",
      "Iter 20: Val loss 1.420, Val took 14.244s\n",
      "Iter 30: Train loss 0.891, It/sec 0.097, Tokens/sec 77.227\n",
      "Iter 30: Val loss 1.476, Val took 16.237s\n",
      "Iter 40: Train loss 0.734, It/sec 0.096, Tokens/sec 77.345\n",
      "Iter 40: Val loss 1.628, Val took 13.777s\n",
      "Iter 50: Train loss 0.732, It/sec 0.079, Tokens/sec 68.118\n",
      "Iter 50: Val loss 1.687, Val took 13.663s\n",
      "[saved] ./stage_cache/aa5051b0d4f20a46/adapters.npz\n",
      "[done] Trial B_lr1e-5_then_3e-5 final adapters at: ./stage_cache/aa5051b0d4f20a46/adapters.npz\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'./stage_cache/aa5051b0d4f20a46/adapters.npz'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Tabular Summary\n",
    "\n",
    "| Trial | Stage | Iters | LR   | Cache   | Runtime (approx) | Notes                                  |\n",
    "| ----- | ----- | ----- | ---- | ------- | ---------------- | -------------------------------------- |\n",
    "| A     | 1     | 50    | 1e-5 | Build   | \\~710s           | Fresh training from scratch            |\n",
    "| A     | 2     | 50    | 5e-5 | Build   | \\~690s           | Resumed from A-Stage-1                 |\n",
    "| B     | 1     | 50    | 1e-5 | **Hit** | \\~0s             | Reused A-Stage-1 checkpoint            |\n",
    "| B     | 2     | 50    | 3e-5 | Build   | \\~690s           | Resumed from shared prefix (A-Stage-1) |\n",
    "\n",
    "### Reuse metrics:\n",
    "- **Total without reuse (naïve):** 4 stages × 50 iters = 200 iterations worth of compute.\n",
    "- With stage reuse:\n",
    "    - A1 trained (50 iters).\n",
    "    - A2 trained (50 iters).\n",
    "    - B1 skipped (reused A1).\n",
    "    - B2 trained (50 iters).\n",
    "        - → Only 150 iters executed.\n",
    "- **Saved:** 50 iterations (~25%).\n",
    "- **Speedup:** 200 / 150 ≈ 1.33×.\n"
   ],
   "id": "e5536fd99d575871"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Stage-Tree Visualization\n",
    "```\n",
    "                ┌─── Stage2: lr=5e-5 (A2)\n",
    "Trial A ── Stage1: lr=1e-5 (A1)\n",
    "                └─── Stage2: lr=3e-5 (B2)\n",
    "```\n",
    "\n",
    "- Stage1 (lr=1e-5, 50 iters) is a shared prefix.\n",
    "    - Trial A uses it → builds fresh.\n",
    "    - Trial B reuses it → cache hit.\n",
    "- Stage2 branches differ:\n",
    "    - Trial A continues with lr=5e-5.\n",
    "    - Trial B continues with lr=3e-5.\n",
    "        - Both had to be trained independently.\n",
    "So the tree has one common trunk (A1/B1) and two diverging branches (A2, B2)."
   ],
   "id": "3fbfdee73257de56"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-20T10:10:51.390447Z",
     "start_time": "2025-09-20T10:10:51.382119Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load as strings first\n",
    "raw = pd.read_csv(\"stage_log.csv\", dtype=str, keep_default_na=False)\n",
    "\n",
    "if raw.empty:\n",
    "    print(\"⚠️ stage_log.csv is empty — no runs recorded yet.\")\n",
    "else:\n",
    "    # Drop rows that are clearly header duplications\n",
    "    mask_header_dup = (raw[\"trial\"] == \"trial\") if \"trial\" in raw else []\n",
    "    if not raw.empty and len(raw) > 0:\n",
    "        try:\n",
    "            mask_header_dup |= (raw.columns.tolist() == raw.iloc[0].tolist())\n",
    "        except IndexError:\n",
    "            pass\n",
    "    raw = raw[~mask_header_dup].copy()\n",
    "\n",
    "    # Convert numeric columns\n",
    "    for col in [\"stage_idx\", \"iters\", \"lr\", \"runtime\", \"cache_hit\", \"cache_miss\"]:\n",
    "        if col in raw:\n",
    "            raw[col] = pd.to_numeric(raw[col], errors=\"coerce\").fillna(0)\n",
    "\n",
    "    print(\"=== Cleaned log ===\")\n",
    "    print(raw)\n",
    "\n",
    "    # Compute reuse metrics\n",
    "    iters_built = raw[raw[\"cache_hit\"] == 0][\"iters\"].sum()\n",
    "    iters_cached = raw[raw[\"cache_hit\"] == 1][\"iters\"].sum()\n",
    "\n",
    "    iters_no_reuse = iters_built + iters_cached\n",
    "    iters_with_reuse = iters_built\n",
    "\n",
    "    saved_iters = iters_no_reuse - iters_with_reuse\n",
    "    pct_saved = (saved_iters / iters_no_reuse) * 100 if iters_no_reuse > 0 else 0.0\n",
    "    speedup = iters_no_reuse / iters_with_reuse if iters_with_reuse > 0 else 1.0\n",
    "\n",
    "    print(\"\\n=== Reuse summary ===\")\n",
    "    print(f\"Iterations without reuse: {iters_no_reuse}\")\n",
    "    print(f\"Iterations with reuse:    {iters_with_reuse}\")\n",
    "    print(f\"Saved iterations:         {saved_iters}  ({pct_saved:.2f}%)\")\n",
    "    print(f\"Speedup:                  {speedup:.2f}×\")\n"
   ],
   "id": "79ea3f9104f9d474",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ stage_log.csv is empty — no runs recorded yet.\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "113a51f819924381",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
